{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1p7-biIXD4_6Vj0pNNILhf-a4cMB11BUG",
      "authorship_tag": "ABX9TyPqLtYAT+OKkvanCJ6Olp1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ragul-n/Gender-bias-in-Indian-cinema/blob/master/word2vec/word2vec_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysrt\n",
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoqD9pYkYAMG",
        "outputId": "6109b597-f9b3-473e-e26a-550ba9c13cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pysrt\n",
            "  Downloading pysrt-1.1.2.tar.gz (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from pysrt) (3.0.4)\n",
            "Building wheels for collected packages: pysrt\n",
            "  Building wheel for pysrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysrt: filename=pysrt-1.1.2-py3-none-any.whl size=13443 sha256=a0d07e4f8e5fac83cf4888793815ab4f7420ad63078c21022a65e5d03682d1b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/a6/ab/4705174e11f44e74d58c14b32edbacbc852644f86658316aef\n",
            "Successfully built pysrt\n",
            "Installing collected packages: pysrt\n",
            "Successfully installed pysrt-1.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFvha_VNNTA7",
        "outputId": "04281f80-d9ab-4640-a790-03744ddd374e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import pysrt\n",
        "import os \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import gensim \n",
        "from gensim.parsing.preprocessing import preprocess_documents,preprocess_string\n",
        "import tqdm\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces,strip_numeric\n",
        "\n",
        "\n",
        "def remove_named_entities(tokens):\n",
        "    # Part-of-speech tag the tokens\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    \n",
        "    # Identify named entities\n",
        "    named_entities = nltk.ne_chunk(pos_tags)\n",
        "    filtered_tokens = [leaf[0] for leaf in named_entities if type(leaf) != nltk.Tree]\n",
        "    \n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "class SentenceGenerator():\n",
        "    def __init__(self, dirname):\n",
        "        self.stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "        self.dirname = dirname\n",
        "\n",
        "        self.subtitles_list=[]\n",
        "        self.sentences=[]\n",
        "\n",
        "\n",
        "        for filename in os.listdir(self.dirname):\n",
        "            # Get the full path of the file\n",
        "            filepath = os.path.join(self.dirname, filename)\n",
        "            \n",
        "            # Check the size of the file\n",
        "            if os.path.getsize(filepath) > 1024*15:\n",
        "                # If the file is larger than the specified size, add it to the list\n",
        "                self.subtitles_list.append(filepath)\n",
        " \n",
        "    def __iter__(self):\n",
        "        for fpath in tqdm.tqdm(self.subtitles_list):\n",
        "\n",
        "            text=pysrt.open(fpath).text.lower()\n",
        "            text= text.replace(\"...\",\" \")\n",
        "            text= text.replace(\"\\n\",\". \")\n",
        "\n",
        "            for sent in sent_tokenize(text):\n",
        "                sent=preprocess_string(sent, filters=[strip_tags, strip_punctuation, strip_multiple_whitespaces,strip_numeric])\n",
        "                sent=remove_named_entities(sent)\n",
        "                #sent=gensim.parsing.preprocessing.remove_stopword_tokens.remove_stopword_tokens(sent)\n",
        "                sent=[token for token in sent if token not in self.stop_words]\n",
        "                \n",
        "                if len(sent)<2:\n",
        "                    continue\n",
        "                yield sent\n",
        "\n",
        "sentences=SentenceGenerator(\"/content/drive/MyDrive/Dataset/subtitles\")"
      ],
      "metadata": {
        "id": "pTuHcfHH6om6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Dataset/subtitles_sents.txt', 'w') as f:\n",
        "    for sent in sentences:\n",
        "        f.write(' '.join(sent) + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTWD519XnrM6",
        "outputId": "69138ee8-71ff-42a7-aed4-85d64eae9a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4168/4168 [4:19:00<00:00,  3.73s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    sents=[line.split() for line in lines]\n",
        "\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "    stemed_sents=[[stemmer.stem(word) for word in sent] for sent in sents]\n",
        "    return  stemed_sents"
      ],
      "metadata": {
        "id": "iE1hAg9WxDMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = read_file(\"/content/drive/MyDrive/Dataset/subtitles_sents.txt\")"
      ],
      "metadata": {
        "id": "oZxxUS4_xEM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg6igMi4xTdG",
        "outputId": "716d7abc-2f9d-4171-9b2f-d6c4c2a68b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4512730"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# init callback class\n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "    Callback to print loss after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        else:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss\n",
        "\n",
        "        model.save('/content/drive/MyDrive/Dataset/word2vec/subtitles_v6')\n",
        "        model.wv.save(\"/content/drive/MyDrive/Dataset/word2vec/subtitles_word_emb_v6.word_vectors\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "monitor = callback()\n",
        "\n",
        "model = gensim.models.Word2Vec(sents, size=200, window=5, min_count=25, workers=2,\n",
        "                               sg=1,negative=5, compute_loss=True,\n",
        "                               iter = 10, callbacks=[monitor])\n"
      ],
      "metadata": {
        "id": "B1DBmYM1Kho5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c661faf0-f089-4329-81e4-0f88e41e0c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 35369760.0\n",
            "Loss after epoch 1: 20944184.0\n",
            "Loss after epoch 2: 11133464.0\n",
            "Loss after epoch 3: 696832.0\n",
            "Loss after epoch 4: 679648.0\n",
            "Loss after epoch 5: 651848.0\n",
            "Loss after epoch 6: 627240.0\n",
            "Loss after epoch 7: 586824.0\n",
            "Loss after epoch 8: 535392.0\n",
            "Loss after epoch 9: 500328.0\n"
          ]
        }
      ]
    }
  ]
}